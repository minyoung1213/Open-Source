#íŒŒì¼ í™•ì¥ìë¡œ ì´ë¯¸ì§€,ì˜ìƒ ìë™ êµ¬ë¶„í•´ì„œ ì¶”ì¶œ, íŒë³„í•˜ë„ë¡ ìˆ˜ì • + tts ì¶”ê°€í•¨
import cv2
import torch
import torch.nn.functional as F
import numpy as np
import json
import pyttsx3
from pose_classifier import PoseClassifier
import os
import sys
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
# ê·¸ ì•„ë˜ì—ì„œ ê²½ë¡œ ë¬¸ì œ ì—†ì´ import ê°€ëŠ¥
from detector.warning_output import WarningOutput
import mediapipe as mp

warner = WarningOutput()  # â† TTS ê°ì²´ ì„ ì–¸
# ì„¤ì •
model_path = "Data/pose_model.pt"
label_map_path = "Data/label_map.json"
input_path = "Data/test1.MOV"   # â† ì—¬ê¸°ë§Œ ë°”ê¾¸ë©´ ë¨ (ì‚¬ì§„ or ì˜ìƒ íŒŒì¼)

# ë¼ë²¨ë§µ & ëª¨ë¸ ë¡œë“œ 
with open(label_map_path, "r") as f:
    raw_label_map = json.load(f)
label_map = {v: k for k, v in raw_label_map.items()}

model = PoseClassifier(input_dim=99, num_classes=len(label_map))
model.load_state_dict(torch.load(model_path, map_location="cpu"))
model.eval()

# MediaPipe ì´ˆê¸°í™” 
mp_pose = mp.solutions.pose
mp_drawing = mp.solutions.drawing_utils
pose = mp_pose.Pose(static_image_mode=True)

# í™•ì¥ì ê¸°ë°˜ìœ¼ë¡œ ì´ë¯¸ì§€ vs ì˜ìƒ êµ¬ë¶„ 
ext = os.path.splitext(input_path)[1].lower()
is_image = ext in ['.jpg', '.jpeg', '.png']

if is_image:
    # ì´ë¯¸ì§€ ì²˜ë¦¬ 
    image = cv2.imread(input_path)
    if image is None:
        raise FileNotFoundError(f"ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")

    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    results = pose.process(rgb)

    if not results.pose_landmarks:
        print("âš ï¸ í¬ì¦ˆ ê°ì§€ ì‹¤íŒ¨: ì‚¬ëŒì´ ì¸ì‹ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    else:
        pose_vector = [v for lm in results.pose_landmarks.landmark for v in (lm.x, lm.y, lm.z)]
        pose_tensor = torch.tensor(pose_vector, dtype=torch.float32).unsqueeze(0)

        with torch.no_grad():
            output = model(pose_tensor)
            probs = F.softmax(output, dim=1)[0]
            pred_class = probs.argmax().item()
            pred_label = label_map[pred_class] 

        print(f"\nğŸ§  ì˜ˆì¸¡ ê²°ê³¼: {pred_label}")
        print("ğŸ“Š í´ë˜ìŠ¤ë³„ í™•ë¥ :")
        for i, p in enumerate(probs):
            print(f"  {label_map[i]:10s}: {p.item():.2%}")

        annotated = image.copy()
        mp_drawing.draw_landmarks(annotated, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)
        cv2.putText(annotated, f"Prediction: {pred_label}", (30, 40),
                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)

        cv2.imshow("Pose Prediction", annotated)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

else:
    # ì˜ìƒ ì²˜ë¦¬
    cap = cv2.VideoCapture(input_path)
    if not cap.isOpened():
        raise FileNotFoundError(f"âŒ ì˜ìƒì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")

    pose = mp_pose.Pose(static_image_mode=False)

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # íšŒì „ + í¬ê¸° ì¡°ì ˆ (ì¶”ê°€)
        frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)  # ì˜ìƒ ëˆ•í˜€ì„œ ë‚˜ì˜¬ ë•Œ íšŒì „
        frame = cv2.resize(frame, (480, 640))  # ì°½ ë„ˆë¬´ í´ ë•Œ ë¦¬ì‚¬ì´ì¦ˆ

        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = pose.process(rgb)

        if results.pose_landmarks:
            pose_vector = [v for lm in results.pose_landmarks.landmark for v in (lm.x, lm.y, lm.z)]
            if len(pose_vector) == 99:
                pose_tensor = torch.tensor(pose_vector, dtype=torch.float32).unsqueeze(0)
                with torch.no_grad():
                    output = model(pose_tensor)
                    probs = F.softmax(output, dim=1)[0]
                    pred_class = probs.argmax().item()
                    pred_label = label_map[pred_class] 

                cv2.putText(frame, f"Prediction: {pred_label}", (30, 40),
                            cv2.FONT_HERSHEY_SIMPLEX, 1.2,
                            (0, 0, 255) if pred_label == "dumping" else (0, 255, 0), 3)
                if pred_label == "dumping":
                    warner.speak_once()
                else:
                    warner.triggered = False

            mp_drawing.draw_landmarks(frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)

        cv2.imshow("Pose Prediction", frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    cv2.destroyAllWindows()
